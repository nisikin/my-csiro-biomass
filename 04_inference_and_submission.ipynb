{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522724ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步骤 0: 环境设置与类定义 ---\n",
      "环境设置完毕。\n",
      "\n",
      "--- 步骤 1: 加载模型 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/77ps8s1x14zg2_q9t7f74t4r0000gn/T/ipykernel_56152/3582582657.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inference_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型 'best_model.pth' 加载成功，并已设置为评估模式。\n",
      "\n",
      "--- 步骤 2: 准备测试数据 ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Sampling_Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/csiro/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Sampling_Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# 执行与训练时完全相同的特征工程\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSampling_Date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mtest_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSampling_Date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    114\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSampling_Date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n\u001b[1;32m    115\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth_sin\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m12\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/csiro/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/csiro/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Sampling_Date'"
     ]
    }
   ],
   "source": [
    "#逻辑错误、废除此文件、仅作为存档\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# 步骤 0: 环境设置与类定义 (必须与训练时完全一致)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 步骤 0: 环境设置与类定义 ---\")\n",
    "\n",
    "# --- 定义模型结构 (直接从训练脚本复制) ---\n",
    "class BiModalModel(nn.Module):\n",
    "    def __init__(self, num_tabular_features, num_targets=5, pretrained=True):\n",
    "        super(BiModalModel, self).__init__()\n",
    "        self.cnn = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        num_cnn_features = self.cnn.fc.in_features\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.tabular_mlp = nn.Sequential(\n",
    "            nn.Linear(num_tabular_features, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.3)\n",
    "        )\n",
    "        total_features = num_cnn_features + 64\n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Linear(total_features, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_targets)\n",
    "        )\n",
    "    def forward(self, image, tabular):\n",
    "        image_features = self.cnn(image)\n",
    "        tabular_features = self.tabular_mlp(tabular)\n",
    "        combined_features = torch.cat((image_features, tabular_features), dim=1)\n",
    "        output = self.fusion_head(combined_features)\n",
    "        return output\n",
    "\n",
    "# --- 定义推理时使用的数据集类 (不包含 target) ---\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, tabular_features, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tabular_data = tabular_features.values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image_path']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        tabular_row = torch.tensor(self.tabular_data[idx], dtype=torch.float)\n",
    "        return image, tabular_row\n",
    "\n",
    "# --- 定义图像变换 (使用验证集的变换) ---\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"环境设置完毕。\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# 步骤 1: 加载训练好的模型\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 步骤 1: 加载模型 ---\")\n",
    "\n",
    "# 确定路径和设备\n",
    "MODEL_PATH = 'best_model.pth'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# !! 关键：先加载训练数据以确定特征数量和 scaler !!\n",
    "# (这是为了让脚本独立，实际项目中 scaler 应该被单独保存和加载)\n",
    "train_full_df = pd.read_csv('train.csv')\n",
    "train_df_wide = pd.pivot_table(train_full_df, index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'], columns='target_name', values='target', aggfunc='mean').reset_index()\n",
    "train_df_wide['Sampling_Date'] = pd.to_datetime(train_df_wide['Sampling_Date'])\n",
    "train_df_wide['Month'] = train_df_wide['Sampling_Date'].dt.month\n",
    "train_df_wide['Month_sin'] = np.sin(2 * np.pi * train_df_wide['Month'] / 12)\n",
    "train_df_wide['Month_cos'] = np.cos(2 * np.pi * train_df_wide['Month'] / 12)\n",
    "train_df_wide = pd.get_dummies(train_df_wide, columns=['State', 'Species'], drop_first=True)\n",
    "target_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'GDM_g', 'Dry_Total_g']\n",
    "feature_cols = [col for col in train_df_wide.columns if col not in target_cols + ['image_path', 'Sampling_Date', 'Month']]\n",
    "\n",
    "# 实例化模型\n",
    "inference_model = BiModalModel(num_tabular_features=len(feature_cols)).to(device)\n",
    "# 加载权重\n",
    "inference_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "# !! 必须设置为评估模式 !!\n",
    "inference_model.eval()\n",
    "\n",
    "print(f\"模型 '{MODEL_PATH}' 加载成功，并已设置为评估模式。\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 步骤 2: 准备测试数据和预处理工具\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 步骤 2: 准备测试数据 ---\")\n",
    "\n",
    "# 加载测试集元数据\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# 执行与训练时完全相同的特征工程\n",
    "test_df['Sampling_Date'] = pd.to_datetime(test_df['Sampling_Date'])\n",
    "test_df['Month'] = test_df['Sampling_Date'].dt.month\n",
    "test_df['Month_sin'] = np.sin(2 * np.pi * test_df['Month'] / 12)\n",
    "test_df['Month_cos'] = np.cos(2 * np.pi * test_df['Month'] / 12)\n",
    "test_df = pd.get_dummies(test_df, columns=['State', 'Species'], drop_first=True)\n",
    "\n",
    "# 确保测试集和训练集的列一致 (有些 species 可能只在训练集出现)\n",
    "for col in feature_cols:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = 0\n",
    "test_df = test_df[feature_cols + ['image_path']] # 保持列顺序一致\n",
    "\n",
    "# !! 关键：使用在 *训练集* 上 fit 好的 scaler 来 transform 测试集 !!\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm', 'Month_sin', 'Month_cos']\n",
    "scaler.fit(train_df_wide[numerical_cols]) # 在全部训练数据上 fit\n",
    "test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n",
    "\n",
    "\n",
    "# 创建测试数据集和加载器\n",
    "IMAGE_DIR = './'\n",
    "test_dataset = InferenceDataset(test_df, IMAGE_DIR, test_df[feature_cols], transform=data_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"测试数据准备完毕。\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 步骤 3: 执行预测\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 步骤 3: 开始预测 ---\")\n",
    "\n",
    "all_predictions = []\n",
    "with torch.no_grad(): # 关闭梯度计算\n",
    "    for images, tabular in tqdm(test_loader, desc=\"预测中\"):\n",
    "        images, tabular = images.to(device), tabular.to(device)\n",
    "        \n",
    "        # 模型输出 log 尺度的预测\n",
    "        log_preds = inference_model(images, tabular)\n",
    "        \n",
    "        # !! 关键：将预测结果还原到原始尺度 !!\n",
    "        preds = np.expm1(log_preds.cpu().numpy())\n",
    "        \n",
    "        all_predictions.append(preds)\n",
    "\n",
    "# 将所有批次的预测结果合并成一个大的 numpy 数组\n",
    "predictions_array = np.concatenate(all_predictions, axis=0)\n",
    "print(\"预测完成。\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 步骤 4: 创建提交文件 (submission.csv)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 步骤 4: 创建提交文件 ---\")\n",
    "\n",
    "# 将预测结果（宽格式）转换为 DataFrame\n",
    "pred_df = pd.DataFrame(predictions_array, columns=target_cols)\n",
    "\n",
    "# 关联 image_path\n",
    "submission_df = pd.concat([test_df[['image_path']].reset_index(drop=True), pred_df], axis=1)\n",
    "\n",
    "# 使用 pd.melt 将宽格式转换为竞赛要求的长格式\n",
    "submission_df = submission_df.melt(id_vars=['image_path'], value_vars=target_cols, var_name='target_name', value_name='target')\n",
    "\n",
    "# 创建最终的 'sample_id'\n",
    "submission_df['sample_id'] = submission_df['image_path'] + '_' + submission_df['target_name']\n",
    "\n",
    "# 按照 sample_submission.csv 的格式选择列并保存\n",
    "final_submission = submission_df[['sample_id', 'target']]\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"提交文件 'submission.csv' 已成功生成！\")\n",
    "print(\"文件预览:\")\n",
    "print(final_submission.head())\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csiro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
