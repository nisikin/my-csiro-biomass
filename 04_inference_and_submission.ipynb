{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522724ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步骤 0: 环境设置与类定义 ---\n",
      "环境设置完毕。\n",
      "\n",
      "--- 步骤 1: 加载模型 ---\n",
      "模型 'best_model.pth' 加载成功，并已设置为评估模式。\n",
      "\n",
      "--- 步骤 2: 准备测试数据 ---\n",
      "测试集行数: 5\n",
      "测试集列: ['sample_id', 'image_path', 'target_name']\n",
      "唯一图片数量: 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'State'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ExceptedGoat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'State'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 136\u001b[39m\n\u001b[32m    134\u001b[39m default_ndvi = train_df_wide[\u001b[33m'\u001b[39m\u001b[33mPre_GSHH_NDVI\u001b[39m\u001b[33m'\u001b[39m].median()\n\u001b[32m    135\u001b[39m default_height = train_df_wide[\u001b[33m'\u001b[39m\u001b[33mHeight_Ave_cm\u001b[39m\u001b[33m'\u001b[39m].median()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m default_state = train_df_wide[\u001b[33m'\u001b[39m\u001b[33mState\u001b[39m\u001b[33m'\u001b[39m].mode()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_df_wide\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mState\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.mode()) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mTas\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    137\u001b[39m default_species = train_df_wide[\u001b[33m'\u001b[39m\u001b[33mSpecies\u001b[39m\u001b[33m'\u001b[39m].mode()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_df_wide[\u001b[33m'\u001b[39m\u001b[33mSpecies\u001b[39m\u001b[33m'\u001b[39m].mode()) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mRyegrass_Clover\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m使用默认值填充测试集元数据:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ExceptedGoat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ExceptedGoat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'State'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# 步骤 0: 环境设置与类定义 (必须与训练时完全一致)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 步骤 0: 环境设置与类定义 ---\")\n",
    "\n",
    "# --- 定义模型结构 (直接从训练脚本复制) ---\n",
    "class BiModalModel(nn.Module):\n",
    "    def __init__(self, num_tabular_features, num_targets=5, pretrained=True):\n",
    "        super(BiModalModel, self).__init__()\n",
    "        self.cnn = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        num_cnn_features = self.cnn.fc.in_features\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.tabular_mlp = nn.Sequential(\n",
    "            nn.Linear(num_tabular_features, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.3)\n",
    "        )\n",
    "        total_features = num_cnn_features + 64\n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Linear(total_features, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_targets)\n",
    "        )\n",
    "    def forward(self, image, tabular):\n",
    "        image_features = self.cnn(image)\n",
    "        tabular_features = self.tabular_mlp(tabular)\n",
    "        combined_features = torch.cat((image_features, tabular_features), dim=1)\n",
    "        output = self.fusion_head(combined_features)\n",
    "        return output\n",
    "\n",
    "# --- 定义推理时使用的数据集类 (不包含 target) ---\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, tabular_features, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tabular_data = tabular_features.values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image_path']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        tabular_row = torch.tensor(self.tabular_data[idx], dtype=torch.float)\n",
    "        return image, tabular_row\n",
    "\n",
    "# --- 定义图像变换 (使用验证集的变换) ---\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"环境设置完毕。\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# 步骤 1: 加载训练好的模型\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 步骤 1: 加载模型 ---\")\n",
    "\n",
    "# 确定路径和设备\n",
    "DATA_DIR = './csiro-biomass'  # 数据目录路径\n",
    "MODEL_PATH = 'best_model.pth'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# !! 关键：先加载训练数据以确定特征数量和 scaler !!\n",
    "# (这是为了让脚本独立，实际项目中 scaler 应该被单独保存和加载)\n",
    "train_full_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "train_df_wide = pd.pivot_table(train_full_df, index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'], columns='target_name', values='target', aggfunc='mean').reset_index()\n",
    "train_df_wide['Sampling_Date'] = pd.to_datetime(train_df_wide['Sampling_Date'])\n",
    "train_df_wide['Month'] = train_df_wide['Sampling_Date'].dt.month\n",
    "train_df_wide['Month_sin'] = np.sin(2 * np.pi * train_df_wide['Month'] / 12)\n",
    "train_df_wide['Month_cos'] = np.cos(2 * np.pi * train_df_wide['Month'] / 12)\n",
    "\n",
    "# !! 关键：在 get_dummies 之前保存默认值，因为之后 State 和 Species 列会被转换 !!\n",
    "default_state = train_df_wide['State'].mode()[0] if len(train_df_wide['State'].mode()) > 0 else 'Tas'\n",
    "default_species = train_df_wide['Species'].mode()[0] if len(train_df_wide['Species'].mode()) > 0 else 'Ryegrass_Clover'\n",
    "default_date = train_df_wide['Sampling_Date'].median()\n",
    "default_ndvi = train_df_wide['Pre_GSHH_NDVI'].median()\n",
    "default_height = train_df_wide['Height_Ave_cm'].median()\n",
    "\n",
    "train_df_wide = pd.get_dummies(train_df_wide, columns=['State', 'Species'], drop_first=True)\n",
    "target_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'GDM_g', 'Dry_Total_g']\n",
    "feature_cols = [col for col in train_df_wide.columns if col not in target_cols + ['image_path', 'Sampling_Date', 'Month']]\n",
    "\n",
    "# 实例化模型\n",
    "inference_model = BiModalModel(num_tabular_features=len(feature_cols)).to(device)\n",
    "# 加载权重（使用weights_only=True以提高安全性）\n",
    "try:\n",
    "    inference_model.load_state_dict(torch.load(MODEL_PATH, map_location=device, weights_only=True))\n",
    "except TypeError:\n",
    "    # 如果PyTorch版本不支持weights_only，则使用旧方法\n",
    "    inference_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "# !! 必须设置为评估模式 !!\n",
    "inference_model.eval()\n",
    "\n",
    "print(f\"模型 '{MODEL_PATH}' 加载成功，并已设置为评估模式。\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 步骤 2: 准备测试数据和预处理工具\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 步骤 2: 准备测试数据 ---\")\n",
    "\n",
    "# 加载测试集（注意：test.csv只有sample_id, image_path, target_name，没有元数据）\n",
    "test_csv = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
    "print(f\"测试集行数: {len(test_csv)}\")\n",
    "print(f\"测试集列: {test_csv.columns.tolist()}\")\n",
    "\n",
    "# 从test.csv中提取唯一的图片（每张图片有5行，对应5个目标）\n",
    "test_images = test_csv[['image_path']].drop_duplicates().reset_index(drop=True)\n",
    "print(f\"唯一图片数量: {len(test_images)}\")\n",
    "\n",
    "# 由于测试集缺少元数据，我们需要使用训练集的统计信息来填充\n",
    "# 默认值已在上面计算（在 get_dummies 之前）\n",
    "\n",
    "print(f\"使用默认值填充测试集元数据:\")\n",
    "print(f\"  Sampling_Date: {default_date}\")\n",
    "print(f\"  Pre_GSHH_NDVI: {default_ndvi:.4f}\")\n",
    "print(f\"  Height_Ave_cm: {default_height:.4f}\")\n",
    "print(f\"  State: {default_state}\")\n",
    "print(f\"  Species: {default_species}\")\n",
    "\n",
    "# 为测试集创建完整的DataFrame（使用默认值）\n",
    "test_df = test_images.copy()\n",
    "test_df['Sampling_Date'] = default_date\n",
    "test_df['State'] = default_state\n",
    "test_df['Species'] = default_species\n",
    "test_df['Pre_GSHH_NDVI'] = default_ndvi\n",
    "test_df['Height_Ave_cm'] = default_height\n",
    "\n",
    "# 执行与训练时完全相同的特征工程\n",
    "test_df['Sampling_Date'] = pd.to_datetime(test_df['Sampling_Date'])\n",
    "test_df['Month'] = test_df['Sampling_Date'].dt.month\n",
    "test_df['Month_sin'] = np.sin(2 * np.pi * test_df['Month'] / 12)\n",
    "test_df['Month_cos'] = np.cos(2 * np.pi * test_df['Month'] / 12)\n",
    "test_df = pd.get_dummies(test_df, columns=['State', 'Species'], drop_first=True)\n",
    "\n",
    "# 确保测试集和训练集的列一致 (有些 species 可能只在训练集出现)\n",
    "for col in feature_cols:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = 0\n",
    "test_df = test_df[feature_cols + ['image_path']]  # 保持列顺序一致\n",
    "\n",
    "# !! 关键：使用在 *训练集* 上 fit 好的 scaler 来 transform 测试集 !!\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm', 'Month_sin', 'Month_cos']\n",
    "scaler.fit(train_df_wide[numerical_cols])  # 在全部训练数据上 fit\n",
    "test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n",
    "\n",
    "# 创建测试数据集和加载器\n",
    "IMAGE_DIR = DATA_DIR  # 图片目录与数据目录相同\n",
    "test_dataset = InferenceDataset(test_df, IMAGE_DIR, test_df[feature_cols], transform=data_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"测试数据准备完毕。\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 步骤 3: 执行预测\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 步骤 3: 开始预测 ---\")\n",
    "\n",
    "all_predictions = []\n",
    "with torch.no_grad(): # 关闭梯度计算\n",
    "    for images, tabular in tqdm(test_loader, desc=\"预测中\"):\n",
    "        images, tabular = images.to(device), tabular.to(device)\n",
    "        \n",
    "        # 模型输出 log 尺度的预测\n",
    "        log_preds = inference_model(images, tabular)\n",
    "        \n",
    "        # !! 关键：将预测结果还原到原始尺度 !!\n",
    "        preds = np.expm1(log_preds.cpu().numpy())\n",
    "        \n",
    "        all_predictions.append(preds)\n",
    "\n",
    "# 将所有批次的预测结果合并成一个大的 numpy 数组\n",
    "predictions_array = np.concatenate(all_predictions, axis=0)\n",
    "print(\"预测完成。\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 步骤 4: 创建提交文件 (submission.csv)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 步骤 4: 创建提交文件 ---\")\n",
    "\n",
    "# 将预测结果（宽格式）转换为 DataFrame\n",
    "pred_df = pd.DataFrame(predictions_array, columns=target_cols)\n",
    "\n",
    "# 关联 image_path（注意：pred_df的行数应该等于唯一图片数量）\n",
    "pred_df['image_path'] = test_df['image_path'].values\n",
    "\n",
    "# 使用 pd.melt 将宽格式转换为竞赛要求的长格式\n",
    "submission_df = pred_df.melt(id_vars=['image_path'], value_vars=target_cols, var_name='target_name', value_name='target')\n",
    "\n",
    "# 创建映射：从 (image_path, target_name) 到 sample_id\n",
    "# 使用原始的test_csv来获取正确的sample_id\n",
    "test_csv_mapping = test_csv.set_index(['image_path', 'target_name'])['sample_id'].to_dict()\n",
    "\n",
    "# 为每行匹配正确的sample_id\n",
    "submission_df['sample_id'] = submission_df.apply(\n",
    "    lambda row: test_csv_mapping.get((row['image_path'], row['target_name']), \n",
    "                                     f\"{row['image_path']}__{row['target_name']}\"), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 按照 sample_submission.csv 的格式选择列并保存\n",
    "final_submission = submission_df[['sample_id', 'target']].copy()\n",
    "\n",
    "# 确保sample_id的顺序与test.csv一致（如果需要的话）\n",
    "# 按照test.csv的顺序排序\n",
    "test_csv_order = test_csv['sample_id'].tolist()\n",
    "final_submission = final_submission.set_index('sample_id').reindex(test_csv_order).reset_index()\n",
    "\n",
    "# 保存提交文件\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"提交文件 'submission.csv' 已成功生成！\")\n",
    "print(f\"提交文件行数: {len(final_submission)}\")\n",
    "print(f\"预期行数: {len(test_csv)}\")\n",
    "print(\"\\n文件预览:\")\n",
    "print(final_submission.head(10))\n",
    "print(\"\\n文件格式验证:\")\n",
    "print(f\"列名: {final_submission.columns.tolist()}\")\n",
    "print(f\"sample_id格式示例: {final_submission['sample_id'].iloc[0]}\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
